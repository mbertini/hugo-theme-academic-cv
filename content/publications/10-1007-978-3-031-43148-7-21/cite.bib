@inproceedings{10.1007/978-3-031-43148-7_21,
  abstract = {The inexorable growth of online shopping and e-commerce demands scalable and robust machine learning-based solutions to accommodate customer requirements. In the context of automatic tagging classification and multimodal retrieval, prior works either defined a low generalizable supervised learning approach or more reusable CLIP-based techniques while, however, training on closed source data. In this work, we propose OpenFashionCLIP, a vision-and-language contrastive learning method that only adopts open-source fashion data stemming from diverse domains, and characterized by varying degrees of specificity. Our approach is extensively validated across several tasks and benchmarks, and experimental results highlight a significant out-of-domain generalization capability and consistent improvements over state-of-the-art methods both in terms of accuracy and recall. Source code and trained models are publicly available at: .},
  address = {Berlin, Heidelberg},
  author = {Cartella, Giuseppe and Baldrati, Alberto and Morelli, Davide and Cornia, Marcella and Bertini, Marco and Cucchiara, Rita},
  bdsk-url-1 = {https://doi.org/10.1007/978-3-031-43148-7_21},
  booktitle = {Proc. of International Conference on Image Analysis and Processing (ICIAP)},
  date-added = {2023-12-06 12:08:58 +0100},
  date-modified = {2023-12-06 12:20:46 +0100},
  doi = {10.1007/978-3-031-43148-7_21},
  isbn = {978-3-031-43147-0},
  keywords = {Vision-and-Language Pre-Training, Open-Source Datasets, Fashion Domain},
  location = {Udine, Italy},
  numpages = {12},
  pages = {245--256},
  publisher = {Springer-Verlag},
  title = {{OpenFashionCLIP}: Vision-and-Language Contrastive Learning with Open-Source Fashion Data},
  url = {https://doi.org/10.1007/978-3-031-43148-7_21},
  year = {2023}
}
